\documentclass[12pt]{article}
\usepackage{lipsum}
\usepackage{authblk}
\usepackage{fancyhdr}

\usepackage{amssymb}
\usepackage{amsmath}

\usepackage[english]{babel}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage[colorlinks=true,
            urlcolor=blue,
            linkcolor=black,
            citecolor=black]{hyperref}
\usepackage[doublespacing]{setspace}
\usepackage{geometry}
  \geometry{
    a4paper,
    right=25mm,
    left=25mm,
    bottom=25mm,
    top=25mm
  }

\usepackage{amsmath}
\usepackage{listings}

\pagestyle{fancy}
\setlength{\headheight}{15pt}

\usepackage{algpseudocode}
\usepackage{algorithm}

\usepackage{cite}
\usepackage[nottoc,numbib]{tocbibind}

\usepackage{../presentation/colordef}
\usepackage{../presentation/lvblisting}

\usepackage{wrapfig}
\usepackage{booktabs}
%\raggedbottom
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}

\tikzstyle{io} = [trapezium, trapezium left angle=70, 
                  trapezium right angle=110, minimum width=3cm, 
                  minimum height=0.5cm, text centered,
                  draw=black, fill=blue!30]
\tikzstyle{process} = [rectangle, minimum width=3cm,
                       minimum height=0.5cm, text centered,
                       draw=black, fill=orange!30]
\tikzstyle{decision} = [rectangle, minimum width=3cm,
                        minimum height=0.5cm, text centered,
                        draw=black, fill=green!30]
\tikzstyle{algo} = [circle, minimum width=0.5cm, draw=black, fill=orange!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\renewenvironment{abstract}{%

\begin{center}
\begin{minipage}{0.9\textwidth}
\rule{\textwidth}{1pt}}
{\par\noindent\rule{\textwidth}{1pt}
\end{minipage}
\end{center}}

\begin{document}

\title{Numerical Methods for solving Eigenvalue-Problems}
\author{Thomas Siskos}
\date{}

\begin{titlepage}
  \begin{center}

  \includegraphics[scale=1.25]{../presentation/hulogo.pdf} \par
  {\scshape\LARGE Humboldt Universit{\"a}t zu Berlin \par}

  {\scshape\Large Seminar Paper\par}

  {\huge\bfseries Numerical Methods for solving Eigenvalue-Problems\par}

\vspace{1cm}

  {\Large\itshape Thomas Siskos (580726)\par}

  {\Large\scshape Numerical Introductory Course\par}

  \vfill
  Supervised by: \par
  {\Large Prof. Dr. Brenda L{\'o}pez Cabrera \par}
  \vfill
  {\large \today\par}
  \end{center}

\end{titlepage}

\tableofcontents
\newpage
\listofalgorithms
\listoffigures
\listoftables
\newpage

\section{Motivation}

\begin{singlespacing}
\begin{abstract}
\textbf{Abstract} \\
\small
Eigenvalues and eigenvectors are often the solution to multidimensional optimization problems, however computing them by hand for anything but trivial matrices is most of the time infeasible or inpractical. To this extend we would like to deploy an automated procedure which yields the correct eigenvectors and eigenvalues. We demonstrate the relevance of eigenvalues and eigenvectors by revising two applications from statistics, Principal Component Analysis and Fisher's Linear Discriminant Analysis, which we follow up by investigating four algorithms suited for eigenvalue problems. Finally we provide a compound solution that takes advantage of each algorithms strengths.  
\end{abstract}
\vspace{3mm}
\end{singlespacing}

For many statistical applications eigenvectors provide a formidable solution. Be it dimensionality reduction in terms of a Principal Component Analysis or classification by Fisher's Linear Discriminant Analysis, both come in the guise of optimization problems. But what are eigenvalues and eigenvectors? 

If $A$ is an $n \times n$ matrix, $v$ is a non-zero vector and $\lambda$ is a scalar, such that

\begin{equation}
\label{eigenvalue-def}
Av = \lambda v
\end{equation}

then $v$ is called an \textit{eigenvector} and $\lambda$ is called an \textit{eigenvalue} of the matrix $A$.
An eigenvalue of A is a root of the characteristic equation,

\begin{equation}
\label{eigenvalue-solve}
det\left(A - \lambda I \right) = 0.
\end{equation}

Geometrically speaking, we require a vector which, when multiplied by matrix $A$, will not get rotated but only elongated by a factor $\lambda$.

When confronted with a high-dimensional data matrix $X \in \mathbb{R}^{n \times m}$ an analyst often wishes to find a lower-dimensional representation, while conserving as much of the structure as possible. One way of achieving this goal is to choose a standardized linear combination of features that aim to maximize the variance of the projection $\delta^{\prime} X$. We can formalize this as

\begin{equation}
	\label{pca_obj}
    	max\ \delta^{\prime} Var \left(X\right) \delta \; s.t. \; \sum \delta_i^2 = 1.
\end{equation}

The solution to this optimization problem is given by
\begin{equation}
\label{pca_sol}
	Y = \Gamma^{\prime} \left(X - \mu\right)
\end{equation}

where $Y \in \mathbb{R}^{n \times m}$ is the matrix of rotations, 
	  $\Gamma \in \mathbb{R}^{m \times m}$ is the matrix of eigenvectors,
	  $\mu \in \mathbb{R}^m$ is the vector of sample means. \cite{MVA}

A second use for Eigenvalues lies in Fisher's Linear Discriminant Analysis for classification problems. In this setting we would like to find the rotation that is best suited to separate the classes. To this effect we try to maximize the between class variance while simoultaneously trying to minimize the within-class variance. This is equivalent to maximizing the so called Rayleigh-coefficient

\begin{equation}
\label{LDA}
max J = \frac{J_b(w)}{J_w(w)} = \frac{w \prime S_{b} w}{w \prime S_{w} w}.
\end{equation}

The matrices for between and within class variance are defined as $S_b = \sum_{c=1}^{C}(\mu_c - \mu)(\mu_c - \mu)\prime$ and $S_w = \sum_{c=1}^{C}\sum_{i \in c}(x_i - \mu_c)(x_i - \mu_c)\prime$

The solution to this maximization problem is given by
\begin{equation}
S_B S_W S_B v = \lambda v,
\end{equation}

which again is an eigenvalue problem.

In the following we will try to extract the eigenvalues of matrices by reformulating any complicated eigenvalue problem into a straightforward one by diagonalizing the matrix in question, without altering the eigenvalues we would like to compute. In section two we provide the mathematical tools, which allow us to do so. We follow these justifications by proposing two main algorithms for computing eigenvalues, first the Jacobi-Method for symmetric matrices, then the QR-Method for arbitrary square matrices in section 3. Additionally, for the QR-Method we define two extensions which try to increase the initial QR-algorithm's speed.
For all algorithms we provide implementations in the \texttt{Python}-programming-language, all plots have been produced with the \texttt{matplotlib}-module and the custom algorithms rely solely on modules from \texttt{Python}'s \texttt{scipy} environment for scientific computing \cite{python, matplotlib, scipy}. In section 4 we will analyse the implemented routines by critically reflecting upon the accuracy of the obtained results as well as their efficiency. In the final section we provide a final algorithm which combines the strengths of the defined procedures by chosing the algorithm that is most fit for the underlying problem.
% ==============================================================================
\section{Similarity Transformations}

In general we want to reformulate the eigenvalue problem of a complicated matrix into an eigenvalue problem of a simple matrix, which yields the same eigenvalues. Simple matrices in our case will be diagonal matrices, since with them it is possible to identify their eigenvalues simply as entries on the main diagonal. Such a transformation that conserves the eigenvalues of a matrix is called a \textit{similarity transformation}. 

Two $n \times n$ matrices $A$ and $B$ are called \textit{similar} if there exists an invertible matrix $P$ such that

\begin{equation}
\label{similarity}
A = P^{-1} B P.
\end{equation}

It is obvious that the similarity relationship is commutative as well as transitive. If $A$ and $B$ are similar, it holds that

\begin{align*}
B - \lambda I &= P^{-1} B P - \lambda P^{-1} I P \notag \\
              &= A - \lambda I.
\end{align*}

 Hence $A$ and $B$ have the same eigenvalues. This fact also follows immediately from the transitivity of the similarity relationship and the fact that a matrix is similar to the diagonal matrix formed from its eigenvalues, as stated in the spectral-decomposition. Important types of similarity transformations are based around orthogonal matrices. If $Q$ is orthogonal and
 
$$ A = Q^{\prime} B Q, $$

$A$ and $B$ are called \textit{orthogonally similar} \cite{NLA}. We will use \textit{orthogonal similarity transformations} to diagonalize matrices we wish to know the eigenvalues of. 

% ==============================================================================
\section{Algorithms}
% ------------------------------------------------------------------------------
\subsection{Jacobi Method}

The \textit{Jacobi-Method} for computing the eigenvalues of a symmetric matrix $A \in \mathbb{R}^{n \times n}$ deploys a sequence of orthogonal similarity transformations that eventually results in
$$ A = P \Lambda P^{-1} \Leftrightarrow \Lambda = P^{-1} A P,$$

where $\Lambda$ is diagonal and $P$ consists of a sequence of matrix multiplications $P = \prod\limits_{k=1}^{K} V_{p_k, q_k}(\theta_k)$ and $V_{p_k, q_k}(\theta_k)$ is of the form proposed in (\ref{givens-rotation}). More specific the \textit{Jacobi iteration} is

\begin{equation}
A^{(k)} = V^{\prime}_{p_k, q_k}(\theta_k) A^{(k-1)} V_{p_k, q_k}(\theta_k),
\end{equation}

where $p_k, q_k$ and $\theta_k$ are chosen such that $A^{k}$ resembles more a diagonal matrix than $A^{k-1}$.  Specifically they will be chosen as to reduce the sum of squares of the off-diagonal elements. As we saw in (\ref{givens-rotation}) it is easy to chose an angle $\theta_k$ in order to introduce a zero in a single Givens rotation. Here we use the rotations in the context of a similarity transformation, so it is a little more complicated.

We require that $a_{pq}^{(k)} = 0$, this implies
\begin{equation}
\label{theta-troubles}
a^{(k-1)}_{pq} (\cos^2\theta - \sin^2\theta) + \left( a^{(k-1)}_{pp} - a^{(k-1)}_{qq} \right) \cos\theta \sin\theta = 0.
\end{equation}
We can use the trigonometric identities
\begin{align*}
\cos(2\theta) &= \cos^2 \theta \sin^2 \theta \\
\sin(2\theta) &= 2 \cos\theta \sin\theta,
\end{align*}

in (\ref{theta-troubles}) we have
$$\tan(2\theta) = \frac{2a^{(k-1)}_{pq}}{a^{(k-1)}_{pp} - a^{(k-1)}_{qq}}.$$

From this we can retrieve the angle and obtain the rotation matrix in each iteration \cite{NLA}.

The algorithm converges if the off-diagonal elements are sufficiently small. The best index pair at a given iteration is the pair $(p, q)$ that satisfies
 
\[
|a^{(k-1)}_{pq}| = \mathop{\max_{i<j}} |a^{(k-1)}_{ij}|.
\]

If this choice is made, the Jacobi Method can be shown to converge \cite{NLA}.

\begin{figure}
\begin{center}
\caption{\href {https://github.com/thsis/NIS18/tree/master/media/plots}{Progress Jacobi-Method}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
  \label{j-plot}  
  \includegraphics[scale=0.6]{../media/plots/jacobi.png}
\end{center}
\end{figure}


Figure \ref{j-plot} visualizes the progress of the \textit{Jacobi}-method on a symmetric $5 \times 5$ matrix. As we can see, in the first iteration the element $a_43$ is eliminated. In the subsequent operations the \textit{Jacobi}-method continues to eliminate any non-zero entries on the off-diagonal until the algorithm convergences after 10 iterations.

\begin{algorithm}
\caption{\href {https://github.com/thsis/NIS18/blob/master/algorithms/eigen.py}{\texttt{jacobi}}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}

\label{j-algo}
\begin{algorithmic}[1]
  \Require symmetric matrix $A$
  \Ensure $0 < precision < 1$
  \Statex \textbf{initialize: } $L \gets A$; $U \gets I$; $L_{max} \gets 1$
  \While{$L_{max} > precision$}
    \State Find indices $i$, $j$ of largest value in lower triangle of $abs(L)$
        \State $L_{max} \gets L_{i,j}$
            \State $\alpha \gets \frac{1}{2}\cdot \arctan(\frac{2A_{i, j}}{A_{i, i}-A_{j, j}})$
    \State $V \gets I$
    \State $V_{i, i}, V_{j, j} \gets \cos \alpha$; $V_{i, j}, V_{j, i} \gets -\sin \alpha, \sin \alpha$
    \State $A \gets V^{\prime} A V$; $U \gets UV$

  \EndWhile\\
  \Return $diag(A),\; U$
\end{algorithmic}
\end{algorithm}

% ------------------------------------------------------------------------------
\subsection{QR-Method}

The most widely used algorithm to extract eigenvalues is the so called \textit{QR}-method. The most important advantage of the \textit{QR}-method over the \textit{Jacobi}-method is that it can be applied to non-symmetric matrices. Note however, that it is simpler for symmetric matrices, since the eigenvalues are real-valued.

The \textit{QR}-method to extract the eigenvalues of a square matrix $A \in \mathbb{R}^{n \times n}$ is performed by first computing the titular \textit{QR} decomposition of $A$.

\begin{equation}
\label{qr_a}
A = QR,
\end{equation}

where $Q$ is an orthogonal and $R$ is an upper triangular matrix. Then define the \textit{QR} iteration as

\begin{equation}
\label{qr-method}
  A^k = Q_{k-1}^{\prime} A_{k-1} Q_{k-1} = R_{k-1}Q_{k-1}
\end{equation}

Note hereby that all matrices in the sequence $\{A_k\}$ share the same eigenvalues, since this procedure is a similarity transformation due to $Q$'s orthogonality. Additionally, in an implementation it is usually preferable to compute the \textit{QR}-iteration in the way shown at the rightmost part of equation (\ref{qr-method}) \cite{NME}. Although, mathematically, each statement is exactly identical there is a practical difference in due to computational imperfections and limited machine precision. The reason being that the computation of $Q_{k-1}^{\prime} A_{k-1} Q_{k-1}$ obviously requires two matrix multiplications whereas the result of $R_{k-1}Q_{k-1}$ can be readily obtained by one. When combining multiple steps over a long sequence of \textit{QR} iterations the additional computations lead to additional rounding errors, which can have an influence on the accuracy of the obtained results. Additionally, less computations lead of course to a faster procedure in general. 

\begin{figure}
\begin{center}
\label{qrm1-plot}
\caption{\href {https://github.com/thsis/NIS18/tree/master/media/plots}{Progress basic QR-Method}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
  \includegraphics[scale=0.6]{../media/plots/qrm1.png}
\end{center}
\end{figure}

Figure \ref{qrm1-plot} visualizes the progress of the basic \textit{QR}-method on the same $5 \times 5$ matrix as in Figure \ref{j-plot}. Compared to the \textit{Jacobi}-method it does not explicitly pick a single element that will be eliminated per iteration. Instead, the \textit{QR}-method extracts the eigenvalues by a process that is called "chasing". By that we mean that alternating steps are being performed, which create non-zero eintries in positions $(i+2, i)$, $(i+3, i)$ and $(i+3, i+1)$ and restore them to zero, as the nonzero entries are moved farther down the matrix \cite{NLA}. We can also see that compared to the \textit{Jacobi}-Method, so far, the \textit{QR}-algorithm lacks in speed. Where the \textit{Jacobi}-method was almost done diagonalizing the matrix in iteration 10, the basic \textit{QR}-algorithm still had multiple non-zero entries left. Thus we would like to make minor improvements on the algorithm's efficiency.
\begin{algorithm}
\caption{\href {https://github.com/thsis/NIS18/blob/master/algorithms/eigen.py}{\texttt{QRM1}}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
\label{qr1-meth}
  \begin{algorithmic}[1]
    \Require square matrix $A$
    \Statex \textbf{initialize: } $conv \gets False$
    \While{not $conv$}
      \State $Q, R \gets$ QR-Factorization of $A$
      \State $A \gets RQ$
      \If{$A$ is diagonal}
        \State $conv \gets \texttt{True}$
      \EndIf
    \EndWhile\\
    \Return $diag\left(A\right),\; Q$
  \end{algorithmic}
\end{algorithm}


\subsubsection{Hessenberg Variant}

In order to speed up the \textit{QR}-method it is advisable to transform the matrix to its upper \textit{Hessenberg} form. A matrix $A$ is of upper \textit{Hessenberg} form if it is upper triangular except for the first subdiagonal, which may be non-zero. In particular $a_{ij} = 0\; \forall i > j + 1$:
$$
\begin{bmatrix}
X & X & X  & \dots &  X & X\\
X & X & X &  \dots &  X & X\\
0 & X & X &  \dots &  X & X\\
0 & 0 & X &  \dots &  X & X \\
\vdots &  \vdots & & \ddots  & \vdots  & \vdots\\
0 & 0 & 0 &  \dots &  X & X \\
\end{bmatrix}$$

A matrix can be reduced to \textit{Hessenberg} form in a finite number of similarity transformations using Householder transformations or Givens rotations. For symmetric matrices the transformation into a \textit{Hessenberg}-form results in a tridiagonal matrix. But even for non-symmetric matrices, the \textit{Hessenberg}-form allows a large saving in subsequent computations. After the transformation we can deploy the previously defined \textit{QR}-method \cite{NLA}.  

\begin{figure}[b]
\centering
\caption{\href {https://github.com/thsis/NIS18/tree/master/media/plots}{Progress Hessenberg-QR-Method}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
  \label{qr2-plot}  
  \includegraphics[scale=0.6]{../media/plots/qrm2.png}
\end{figure}

Figure \ref{qr2-plot} visualizes the progress of the \textit{Hessenberg} variant of the \textit{QR}-method. In order to make it comparable to the previous algorithms the same $5 \times 5$-dimensional matrix is being evaluated. We can readily see, that the transformation to \textit{Hessenberg}-form results in a tridiagonal matrix. This facilitates computations and explains the vastly improved resulting matrix after 10 iterations compared to the basic \textit{QR}-method. However, it still does not match the progress of the \textit{Jacobi}-method after the same number of iterations.


\begin{algorithm}
\caption{\href {https://github.com/thsis/NIS18/blob/master/algorithms/eigen.py}{\texttt{QRM2}}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
\label{qr2-meth}
\begin{algorithmic}[1]
  \Require square matrix $A$
  \State $A \gets \texttt{hessenberg(}A\texttt{)}$
  \State continue with: \Call {QRM1} A
\end{algorithmic}
\end{algorithm}

\subsubsection{Accelerated Variant}

We could already improve the \textit{QR}-method and cut down on computational cost. However, we still cannot match the results of the \textit{Jacobi} method. To this effect we present the final adjustment on the \textit{QR}-method to improve convergence speed. The general idea is, that we deliberately create an additional zero entry on the main diagonal by subtracting a scalar on each element, perform the \textit{QR}-iteration and finally undo the subtraction. In particular, we define

\begin{equation}
\label{qrm3-prop}
T^{m} = \begin{bmatrix}

\alpha^{m}_1 & \beta^{m}_1  & 0            & 0            & \dots            & 0                                  \\
\beta^{m}_1  & \alpha^{m}_2 & \beta^{m}_2                                                                         \\
0            & \beta^{m}_2  & \alpha^{m}_3 & \beta^{m}_3  &                  & \vdots                             \\
             &              & \ddots       & \ddots       & \ddots                                                \\
             &              &              &              & \beta^{m}_{n-2}  & \alpha^{m}_{n-1} & \beta^{m}_{n-1} \\
0            &              &              &              &                  & \beta^{m}_{n-1}  & \alpha^{m}_n    \\

\end{bmatrix}
\end{equation}


\begin{align*}
T^{m}       &= T - t _{n, n} I \\
T^{m}       &= QR \\
T^{m+1}     &= T^{m} + t _{n, n} I
\end{align*}

After this we can define the accelerated iteration step as
\begin{align}
\label{qrm3}
R_m &= Q^{\prime}_m\left(T_m - \alpha_n^{m}I\right) \\
T_{m+1} &= Q^{\prime}_m\left(T_m - \alpha_n^{m}I\right)Q_m + \alpha_n^{m}I \\
        &= Q^{\prime}_m T_m Q_m
\end{align}

Again $T_{m+1}$ is similar to $T_m$. 

\begin{figure}[b]
\centering
\caption{\href {https://github.com/thsis/NIS18/tree/master/media/plots}{Progress Accelerated QR-Method}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
  \label{qr3-plot}
  \includegraphics[scale=0.6]{../media/plots/qrm3.png}
\end{figure}

Figure \ref{qr3-plot} visualizes the progress of the accelerated \textit{QR}-method. For comparability, the matrix used is the same $5 \times 5$ matrix as before. Most notably is, that the accelerated method still performs worse than the \textit{Jacobi} method. So far results seem similar compared to the \textit{Hessenberg} variant of the \textit{QR}-method. The case can be made that the accelerated method performs considerably better than the basic \textit{QR}-method and slightly better than the \textit{Hessenberg} variant after 10 iterations than the. Howbeit, this claim warrants further analysis. 

\begin{algorithm}
\begin{algorithmic}[1]
\caption{\href {https://github.com/thsis/NIS18/blob/master/algorithms/eigen.py}{\texttt{QRM3}}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}

\Require square matrix $A \in \mathbb{R}^{p \times p}$
\State $T \gets \texttt{hessenberg}(A),\ conv \gets False$
\While{not $conv$}
    \State $Q, R \gets$ QR-Factorization of $T - t_{p-1, p-1} I$
    \State $T \gets RQ + t_{p-1, p-1}I$
    \If{$T$ is diagonal}
        \State $conv \gets True$
    \EndIf
\EndWhile\\
\Return $diag\left(T\right),\; Q$
\end{algorithmic}
\end{algorithm}

% ==============================================================================
\section{Analysis}

In order to test the quality of our implemented algorithms we will follow a two pronged approach. First, we will scrutinize the quality of the obtained results. Once we can be sure that the implemented routines are reliable or, to be more precise, under which conditions our routines are reliable we can analyze their convergence behavior and compare the efficiency accross algorithms in terms of computational cost.
% ------------------------------------------------------------------------------
\subsection{Accuracy}

If we wish to investigate the accuracy of our algorithms, we first need to define what we mean by that. We set up an environment of unit tests that compare the absolute difference between the vector of true eigenvalues of a matrix and the vector of computed eigenvalues by our custom algorithms. If any element of this $L_1$-norm is larger than a threshold of $10^{-5}$ we consider the test as failed. In order to obtain matrices with known eigenvalues we will reverse the spectral decomposition, which states that every square, symmetric matrix $A \in \mathbb{R}^{m \times m}$ can be written as
\begin{equation}
\label{spectral-decomp}
A = \Gamma \Lambda \Gamma^{\prime} = \sum\limits_{j=1}^m \lambda_j \gamma_j \gamma^{\prime}_j,
\end{equation}
where $\Lambda = diag(\lambda_1, \dots, \lambda_m)$ is a diagonal matrix with the eigenvalues of $A$ on its main diagonal and where $\Gamma=(\gamma_1, \dots, \gamma_m)$ is the matrix containing the associated eigenvectors \cite{MVA}. This means we can draw a random vector of eigenvalues $\lambda_{true}$ and a random orthogonal matrix to construct a test matrix $A_{test}$. Once the matrix $A_{test}$ is initialized, we can plug it into the eigenvalue-routine we wish to test.


\begin{table}
\centering
\caption{\href {https://github.com/thsis/NIS18/tree/master/tests/tests_eigen.py}{Failed unit tests accross matrix-sizes}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
\label{acc-res}
\begin{tabular}{ccrrrrr}
\toprule       
Algorithm & Maximum number & \multicolumn{5}{c}{Dimension}\\
\cline{3-7}
          & of iterations &    3 &    4 &    5 &     6 &     7 \\
\toprule
\bottomrule
\textit{Jacobi} & - &    0 &    0 &    0 &     0 &     0 \\
\textit{QRM1}   & 10 &  785 &  952 &  994 &   999 &  1000 \\
                & 100 &  110 &  224 &  349 &   474 &   565 \\
                & 1000 &    7 &   16 &   33 &    38 &    45 \\
                & 10000 &    1 &    1 &    1 &     1 &     3 \\
                & 100000 &    0 &    0 &    0 &     0 &     0 \\
\textit{QRM2}   & 10 &  826 &  975 &  997 &  1000 &  1000 \\
                & 100 &  111 &  191 &  360 &   467 &   602 \\
                & 1000 &    9 &   16 &   22 &    35 &    58 \\
                & 10000 &    0 &    0 &    3 &     2 &     0 \\
                & 100000 &    0 &    0 &    0 &     0 &     0 \\
\textit{QRM3}   & 10 &  251 &  616 &  896 &   983 &   997 \\
                & 100 &   29 &   52 &  116 &   222 &   311 \\
                & 1000 &    6 &    2 &    8 &    19 &    21 \\
                & 10000 &    0 &    0 &    1 &     1 &     3 \\
                & 100000 &    0 &    0 &    0 &     0 &     0 \\
\bottomrule
\end{tabular}
\end{table}


The number of failures out of 1000 unit tests on matrices of different dimensions are catalogued in Table \ref{acc-res}. Most notably, we see that the \textit{Jacobi}-method outperforms any of the \textit{QR}-methods in terms of accuracy. This is mostly due to its convergence criterion being defined differently than the convergence criterion of the \textit{QR}-methods. Simply put, the \textit{Jacobi}-method will always converge in this setup. Also keep in mind, that since the \textit{Jacobi}-method is only allowed for symmetric matrices, it essentially eliminates two non-zero off-diagonal elements at once. Comparing only the \textit{QR}-methods amongst each other one can clearly see, that the quality of the obtained results increases, the more willing an analyst is to wait. Convergence is slower for the \textit{QR}-methods and a premature stop leads to poor eigenvalues. One the other hand, if the \textit{QR}-methods converge, they unanimously pass all tests. It would therefore be possible to always achieve an arbitrary grade of precision by setting the maximum number of iterations to infinity.

% ------------------------------------------------------------------------------
\subsection{Efficiency}


\begin{figure}
\centering
\caption{\href {https://github.com/thsis/NIS18/blob/master/tests/tests_eigen.py}{Unit-tests: Iterations}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
  \label{algo_efficiency}
  \includegraphics[width=\textwidth, height=5cm]{../media/plots/iterations_boxplot.png}
\end{figure}

Now that we can be confident that each of the routines is able to offer accurate results we are interested in which method is most appropriate for which kind of matrix. Especially, we are interested in convergence speed. Figure \ref{algo_efficiency} shows the distribution of steps performed before the algorithms reached convergence or their maximum allowed number of iterations. Again, the \textit{Jacobi}-method outperforms every \textit{QR}-method. For matrices of small dimensions, i.e. eleven or less, the accelerated \textit{QR}-method outperforms the other \textit{QR}-methods. However, for larger matrices the \textit{Hessenberg} variant seems to be slightly more appropriate, since the accelerated method tends to have a wider range of needed steps. Additionally, the accelerated method's distribution has a wider tail whenever the dimension of the test matrices is larger than eleven. This means that high waiting times, operating on the same matrix, are more likely to occur using the \textit{accelerated} variant compared to the \textit{Hessenberg} variant or even the basic variant. Surprisingly, there seems to be no striking difference between the distributions of the basic and the \textit{Hessenberg} variants. Looking closely, the only effect of performing a \textit{Hessenberg} transformation prior to running a \textit{QR}-algorithm on a test matrix is to mitigate outliers in terms of computation steps needed before convergence. And even that is true, only for large matrices. On the other hand, it does not seem to incurr systematically higher costs.  

% ==============================================================================
\section{Conclusion}
\begin{figure}
\centering
\caption{\href {https://github.com/thsis/NIS18/tree/master/algorithms/eigen.py}{Decision process of final eigenvalue routine}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
\begin{tikzpicture}
\node (in1) [io] {Input: matrix A};
\node (dec_sym) [decision, below of=in1, yshift=-0.25cm] {Is A symmetric?};
\node (algo1) [algo, below of=dec_sym, xshift=-2cm, yshift=-0.5cm] {\texttt{Jacobi}};
\node (dec_small) [decision, below of=dec_sym, xshift=2cm, yshift=-0.5cm] {rows of A $\leq$ 11?};
\node (algo2) [algo, below of=dec_small, xshift=-1cm, yshift=-0.5cm] {\texttt{QRM3}};
\node (algo3) [algo, below of=dec_small, xshift=1cm, yshift=-0.5cm] {\texttt{QRM2}};

\draw [arrow] (in1) -- (dec_sym);
\draw [arrow] (dec_sym) -- node[anchor=east] {yes} (algo1);
\draw [arrow] (dec_sym) -- node[anchor=west] {no} (dec_small);
\draw [arrow] (dec_small) -- node[anchor=east] {yes} (algo2);
\draw [arrow] (dec_small) -- node[anchor=west] {no} (algo3);
\end{tikzpicture}
\end{figure}

We have demonstrated the practical importance of eigenvalues in the context of Principal Component Analysis and Fisher's Linear Discriminant Analysis. For computing the Eigenvalues we have shown various numerical procedures. We examinated the \textit{Jacobi}- and the \textit{QR}-method, as well as a selection of simple improvements towards the basic \textit{QR}-method. Finally, we would like to present a compound solution which combines the strengths of all presented algorithms and automatically picks the procedure that fits best to the problem. For obvious reasons, it is best to chose the \textit{Jacobi}-method, whenever applicable. That is, when the input-matrix is symmetric. Whenever the matrix is not symmetric, a second decision based on the matrix' size will determine which \textit{QR}-method will be used. If the matrix' dimension is smaller than eleven it will chose the accelerated \textit{QR} method. Otherwise, the \textit{Hessenberg} variant of the \textit{QR}-method will be deployed. 

% ==============================================================================
\newpage
\section{Appendix}
\subsection{Householder-Reflections}

Our goal is still to diagonalize a matrix in order to programmatically extract its eigenvalues. So far we have seen that there exist such transformations that conserve the eigenvalues of a given matrix. However, we require transformations that, additionally, eliminate non-zero entries on the off-diagonal elements of said matrix. A greedy technique, that eliminates all but the first elements of a vector is proposed in the form of Householder-Reflections.

Let $u$ and $v$ be orthonormal vectors and let $x$ be a vector in the space spanned by $u$ and $v$, such that
$$x = c_1 u + c_2 + v$$ 
for some scalars $c_1$ and $c_2$. The vector 
$$\tilde{x}=-c_1 u + c_2 v$$ 
is a \textit{reflection} of x through the line difined by the vector u. Now consider the matrix

\begin{equation}
P = I - 2 uu^{\prime}.
\end{equation}
Note that
\begin{align*}
Px &= c_1 u + c_2 v - 2c_1 uuu^{\prime} - 2 c_2 v uu^{\prime} \\
   &= c_1 u + c_2 v - 2c_1 u^{\prime}uu - 2 c_2 u^{\prime} v u \\
   &= -c_1 u + c_2 v\\
   &= \hat{x}.
\end{align*}

The matrix $P$ is called a reflector. The usefulness of Householder-Reflections stems from the fact that it is easy to transform a vector of the form

$$x = (x_1, x_2, \dots, x_n)$$

into a vector
$$\hat{x} = (\hat{x}_1, 0, \dots, 0).$$

If $Qx = \hat{x}$, then $||x||_2 = ||\hat{x}||_2$ and thus $\hat{x_1} = \pm ||x||_2$, since it is the only non-zero entry. To construct the reflector let
\begin{equation}
\label{house-con}
v = (x_1 + sign(x_1)||x||_2, x2, \dots, x_n)
\end{equation}
and $u = \frac{v}{||v||_2}$ \cite{NLA}. We use the $sign$-function, which simply returns the sign of its argument in order to avoid the numerical problem known as \textit{catastrophic cancellation}. It can occur when adding two very close, but different, floating point numbers of differing signs. In some unfortunate cases both of these numbers get represented by the same computer number and, because of their opposing signs cancel each other out. In our case this would mean, that we reflect the vector onto the origin. Fortunately, by making use of the \textit{sign} function we can make sure that both summands will share the same sign, thus mitigating any concerns about catastrophic cancellation.

We use reflectors to compute the so called $QR$ factorization of an aribitrary square matrix $A \in \mathbb{R}^{n \times n}$.

\begin{equation}
\label{QR-prop}
A = QR
\end{equation}

where $Q$ is orthogonal and $R$ is upper triangular. We use Householder transformations to reflect the $i^{th}$ column and produce zeros below the $(i, i)$ element. The QR-factorization of a matrix $A \in \mathbb{R}^5$ would therefore consist of five Householder-reflections with $Q=P_5 P_4 P_3 P_2 P_1$. The number of computations for the \textit{QR} factorization in this fashion is $2n^3 / 3$ multiplications and $2n^3 / 3$ additions \cite{NLA}.  
% ------------------------------------------------------------------------------
\subsection{Givens-Rotations}

\begin{wrapfigure}[7]{r}{0.35\textwidth}
\centering
\caption{\href {https://github.com/thsis/NIS18/blob/master/media/plots}{Rotation of \textit{x}}  \protect\includegraphics[scale=0.05]{qletlogo.pdf}}
\includegraphics[scale=0.5]{../media/plots/givens.png}
\end{wrapfigure}

Another way of forming the \textit{QR}-factorization is by using orthogonal transformations which rotate a vector in a way such that a specific element becomes 0 and only one other element in the vector being changed. These transformations are called \textit{Givens transformations, Givens Rotations} or \textit{Jacobi transformations}

Using orthogonal transformations we can also rotate a vector in such a way that a specified element becomes 0 and only one other element in the vector is changed. The basic idea can be seen in a two-dimensional space. We wish to rotate the vector $x = (x_1, x_2)$ to $\tilde{x} = (\tilde{x_1}, 0)$ as with a reflector.

It is easy to see that the orthogonal matrix 
$$Q=\begin{bmatrix}
\cos\theta & \sin\theta \\
-\sin\theta & \cos\theta
\end{bmatrix}$$

performs the desired rotation, if $\cos\theta = \frac{x_1}{||x||_2}$ and $\sin\theta = \frac{x_2}{||x||_2}$

In general, we can construct an orthogonal $matrix V_{pq}$, that will transform the vector $$x = (x_1,\dots, x_p, \dots x_q, \dots, x_n)$$ to $$\tilde{x} = (x_1,\dots, \tilde{x}_p, \dots 0, \dots, x_n)$$. The matrix that does this is
\begin{equation}
\label{givens-rotation}
V_{pq}(\theta) = \begin{bmatrix}
                      1 \\
                        & \ddots \\
                        &        & \cos\theta    &        & \sin\theta  \\
                        &        &               & \ddots     \\
                        &        & -\sin\theta   &        & \cos\theta   \\
                        &        &               &        &            &  \ddots \\
                        &        &               &        &            &         & 1 \\                      
                 \end{bmatrix}
\end{equation}
	where $\cos\theta = \frac{x_p}{||x||}$ and $\sin\theta = \frac{x_q}{||x||}.$
	
A rotation matrix is therefore the same as an identitiy matrix, in which we change four elements \cite{NME}. We will use Givens rotations primarily in the Jacobi-Method.


\subsection{Eigenvalue Routines}
  \lstinputlisting[language=Python]{../algorithms/helpers.py}
  \lstinputlisting[language=Python]{../algorithms/eigen.py}
  \newpage
\subsection{Analysis: Figures}
  \lstinputlisting[language=Python]{../analysis/analysis.py}
  \newpage    
\subsection{Analysis: Unit tests}
  \lstinputlisting[language=Python]{../tests/tests_eigen.py}
  \newpage
% ==============================================================================
\bibliographystyle{apalike}
\bibliography{references}

\end{document}